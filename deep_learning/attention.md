1. attention是在seq2seq的基础上给隐藏层变量添加了新的信息，这些信息=输入层各影藏层的加权和
2. NLP中利用样本增强，对原始句子进行反转，能取得一定的效果

